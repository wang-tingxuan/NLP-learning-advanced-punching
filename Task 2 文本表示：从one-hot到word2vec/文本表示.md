## 1.学习词袋模型概念：离散、高维、稀疏
离散：在信息检索中，词袋模型假定对于一个文本，忽略其词序和语法、句法，将其仅仅看做是一个词合，	或者说是词的一个组合，文本中每个词的出现都是独立的，不依赖于其他词是否出现，或者说当这篇文章	的作者在任意一个位置选择一个词汇都不受前面句子的影响而独立选择的[9]。

高维：当文本数量多时，进而会导致词汇表增大，最后在表示时，维数也就增加了，因此高维；

稀疏：高维中，一次出现的字或词只是词汇表中较少的一部分，有由于词袋的形式，未出现的词对应的记为0，最终一文本在高维的表示中0占多数，进而导致稀疏。
## 2.学习分布式表示概念：连续、低维、稠密
连续：原本是相互独立的，映射到同一个固定长度的向量后，可通过判断距离来判断相似性；

低维：在原本高维中，映射到一个固定且维度较低的词向量空间中；

稠密：相对于原本的高维中，只有一个非零量，而在分布式表示中，有较多非零量。
## 3.理解word2vec词向量原理并实践，来表示文本
word2vec（word to vector）是一个将单词转换成向量形式的工具。
作用：
word2vec适合用作序列数据的分类，聚类和相似度计算。有用作app下载推荐系统中的，也有用在推荐系统和广告系统上的，也可以用在机器人对话类别判决系统上。
算法：
首先这是一个逻辑回归（分类）问题，使用最大似然估计。 在已知历史单词，要最大化下一个单词出现的概率，使用softmax函数做分类
代码：
```python 
import gensim
import pandas as pd
import numpy as np
vector_size = 100
 
def sentence2list(sentence):
    return sentence.strip().split()
 
"""读取数据"""
print("data read begin...")
train_data = pd.read_csv('train.csv')
test_data = pd.read_csv('test.csv')
train_data.drop(columns=['article','id'], inplace = True)
test_data.drop(columns=['article'], inplace = True)
print("data read end...")
 
"""准备数据"""
print("准备数据... ")
sentences_train = list(train_data.loc[:, 'word_seg'].apply(sentence2list))
sentences_test = list(test_data.loc[:, 'word_seg'].apply(sentence2list))
sentences = sentences_train + sentences_test
print("准备数据完成! ")
 
print("开始训练...")
model = gensim.models.Word2Vec(sentences=sentences, size=vector_size, window=5, min_count=5, workers=8, sg=0, iter=5)
print("训练完成! ")
```

## 4.word2vec 中的数学原理详解
[链接](https://blog.csdn.net/itplus/article/details/37969519)

## 5.word2vec原理推导与代码分析
[链接](http://www.hankcs.com/nlp/word2vec.html)
