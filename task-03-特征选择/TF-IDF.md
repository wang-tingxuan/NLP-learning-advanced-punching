# 任务

1.TF-IDF原理。

2.文本矩阵化，使用词袋模型，以TF-IDF特征值为权重。（可以使用Python中TfidfTransformer库）

3.互信息的原理。

4.使用第二步生成的特征矩阵，利用互信息进行特征筛选。

参考资料

[文本挖掘预处理之TF-IDF：文本挖掘预处理之TF-IDF - 刘建平Pinard - 博客园] https://www.cnblogs.com/pinard/p/6693230.html 

[使用不同的方法计算TF-IDF值：使用不同的方法计算TF-IDF值 - 简书] https://www.jianshu.com/p/f3b92124cd2b

[sklearn-点互信息和互信息：sklearn：点互信息和互信息 - 专注计算机体系结构 - CSDN博客] https://blog.csdn.net/u013710265/article/details/72848755

[如何进行特征选择（理论篇）机器学习你会遇到的“坑”：如何进行特征选择（理论篇）机器学习你会遇到的“坑”] https://baijiahao.baidu.com/s?id=1604074325918456186&wfr=spider&for=pc

# 1 TF-IDF原理
TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文件频率)，是一种用于资讯检索与资讯探勘的常用加权技术。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章.。

## 1.1 词频 (term frequency, TF)
TF指的是某一个给定的词语在该文件中出现的次数。这个数字通常会被归一化(一般是词频除以文章总词数), 以防止它偏向长的文件。（同一个词语在长文件里可能会比短文件有更高的词频，而不管该词语重要与否。）

【注意】： 一些通用的词语对于主题并没有太大的作用, 反倒是一些出现频率较少的词才能够表达文章的主题, 所以单纯使用是TF不合适的。权重的设计必须满足：一个词预测主题的能力越强，权重越大，反之，权重越小。所有统计的文章中，一些词只是在其中很少几篇文章中出现，那么这样的词对文章的主题的作用很大，这些词的权重应该设计的较大。IDF就是在完成这样的工作.

公式：
TFw=在某一类中词条w出现的次数该类中所有的词条数目 TF_w = \frac{在某一类中词条w出现的次数}{该类中所有的词条数目}
TF 
w
​	
 = 
该类中所有的词条数目
在某一类中词条w出现的次数
​	
 

2.2 逆向文件频率 (inverse document frequency, IDF)
IDF的主要思想是：如果包含词条t的文档越少, IDF越大，则说明词条具有很好的类别区分能力。某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。
公式:
IDF=log(语料库的文档总数包含词条w的文档数+1) IDF = log(\frac{语料库的文档总数}{包含词条w的文档数+1})
IDF=log( 
包含词条w的文档数+1
语料库的文档总数
​	
 )

分母之所以要加1，是为了避免分母为0

2.3 TF-IDF
某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。
TF−IDF=TF∗IDF TF−IDF=TF∗IDF
TF−IDF=TF∗IDF

3. 文本矩阵化，使用词袋模型，以TF-IDF特征值为权重。
3.1 使用TfidfTransformer
除了考量某词汇在文本出现的频率，还关注包含这个词汇的所有文本的数量，能够削减高频没有意义的词汇出现带来的影响, 挖掘更有意义的特征，相比之下，文本条目越多，Tfid的效果会越显著

3.2 使用CountVectorizer
只考虑词汇在文本中出现的频率
--------------------- 
版权声明：本文为CSDN博主「黑桃5200」的原创文章，遵循CC 4.0 by-sa版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/Heitao5200/article/details/88292437
